{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11700607",
   "metadata": {},
   "source": [
    "Hej gruppen!\n",
    "\n",
    "I början antecknade jag för hand och inte speciellt organiserat. Det märks lite när det gäller frånvaron av hållpunkter (vilken sektion av videon jag hämtat informationen ifrån), men jag tror inte det gör så stor skillnad från er såtillvida ni inte vill kolla upp något i videon. Det är inte renskrivet på något sätt, men ambitionen har varit att skriva så att vi kan använda den som en lathund när vi väl sätter igång. \n",
    "\n",
    "Det är många upprepningar av kod för att det lättare skulle gå att få en överblick om vi skulle hoppa in någonstans mitt i. I slutet tröttnade jag dock vid något tillfälle och då har jag bara skrivit in referenser (den första biten kod) till de celler som ska användas.\n",
    "\n",
    "Avnittet Understandning the Drawbacks of Using Deep Neural Networks with Images hoppade jag över helt och hållet.\n",
    "\n",
    "Vill ni kolla på videon heter den Image Classification with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e1b0e",
   "metadata": {},
   "source": [
    "Color images has 3 channels (R, G, B) and gray scale images 1 (pixel intensity). The number of channels specifies the number of elements in the 3rd dimension (the first two are height and width but this can vary).\n",
    "Grayscale (3,3,1)\n",
    "Color (3,3,3)\n",
    "\n",
    "Deep learning frameworks usually deal with a list of images in one 4d tensor (10,3,3,3) where the first dimension is the batch dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286a66b",
   "metadata": {},
   "source": [
    "### Image Pre-processing Methods\n",
    "#### Common techniques to improve CNN performance\n",
    "    1. Uniform Aspect Ratio\n",
    "    2. Uniform Image Size\n",
    "    3. Mean and Perturbed Images\n",
    "    4. Normalized Image Inputs\n",
    "    5. Dimensionality Reduction\n",
    "    6. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7f665",
   "metadata": {},
   "source": [
    "#### Uniform Aspect Ratio\n",
    "The ratio of the width of your image to the height of the image. Scale all \n",
    "images to have the same ration.\n",
    "Crop images to square shape. Do a center crop to extract the most important part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ec73f",
   "metadata": {},
   "source": [
    "#### Uniform Image Size\n",
    "All images have to be of the same size. Down-scaling and up-scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd5488",
   "metadata": {},
   "source": [
    "#### Mean and Perturbed Images\n",
    "##### Mean image:\n",
    "Calculate average pixel value across the entire training data set. Eg. faces is usually centered.\n",
    "Mean image technique will help yout CNN detect patterns.\n",
    "##### Perturbed image:\n",
    "Intentionally distort pixels by varying them from the mean image.\n",
    "Perturbed image technique can make your CNN more robust by preventing it from focusing only on the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c048f",
   "metadata": {},
   "source": [
    "#### Normalized Image Inputs\n",
    "Center your pixel values around 0 mean. You normalize eache pixel by subtracting the mean pixel value across all of your training data or single batch, and dividing by the standard deviation. This ensures that each pixel has a similar data distribution. After normalization you might choose to rescale your pixel values to be in th 0-1 range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48d3ba",
   "metadata": {},
   "source": [
    "#### Deimensionality Reduction\n",
    "RGB => grayscale reduces 3 channels to 1. Training completes faster and helps extract the latent of significant features from your underlying data so thar your neural network does not have to deal with irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e0e99",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "Perturbed images are a form of data augmentation. Others are:\n",
    "- scaling,\n",
    "- rotating or\n",
    "- perform affine transforms of the input image.\n",
    "Reduces the risk of overfitting your CNN on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c01d90",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "imread() missing 1 required positional argument: 'fname'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aa01cbc4c304>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Läsa in bilden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Reshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: imread() missing 1 required positional argument: 'fname'"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "mpimg.imread(img)  # Läsa in bilden\n",
    "\n",
    "# Reshape\n",
    "reshape(img.shape[0],-1)  # Samma höjd, men ändrat bredd och kanalvärde\n",
    "\n",
    "# Resize\n",
    "skimage.transform.resize(img, (500,500))  # Ändrar även proportioner\n",
    "\n",
    "# Ratio\n",
    "aspect_ratio_original = img_original.shape[1]/float(img_original.shape[0])\n",
    "aspect_ratio_resized = img_resized.shape[1]/float(img_resized.shape[0])\n",
    "\n",
    "# Rescale\n",
    "rescaled = skimage.transform.rescale(resized, (1.0, aspect_ratio_original))\n",
    "\n",
    "# RGB => BGR\n",
    "change_channel_order = img[:, :, (2,1,0)]\n",
    "\n",
    "# Gray\n",
    "to_gray = skimage.color.rgb2gray(img)\n",
    "\n",
    "# As float\n",
    "as_float = skimage.img_as_float(skimage.io.imread(img)).astype(np.float32)\n",
    "\n",
    "# Crop\n",
    "def crop(image, cropx, cropy):  # (img,) width and height, eg. 256, 256\n",
    "    y, x, c = image.shape\n",
    "    startx = x // 2 - (cropx // 8)  # Vertictal line halfway? \n",
    "    starty = y // 3 - (cropy // 4)  # Horizontal line on third from top? \n",
    "    stopx = startx + cropx\n",
    "    stopy = starty + 2 * cropy\n",
    "    return image[starty:stopy, startx:stopx]\n",
    "\n",
    "# Show picture\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.title('Something')\n",
    "plt.imshow(img_name)\n",
    "\n",
    "# Make noise in image\n",
    "from skimage.util import random_noise\n",
    "sigma = 0.155\n",
    "noisy = random_noise(img, var=sigma**2)\n",
    "\n",
    "# Remove noise from image\n",
    "from skimage import denoise_tv_chambolle, denoise_bilateral, denoise_wavelet, estimate_sigma\n",
    "\n",
    "#sigma_est shows the noise value\n",
    "sigma_est = estimate_sigma(img, multichannel=True, average_sigmas=True)\n",
    "\n",
    "# Total variation filter\n",
    "plt.imshow(denoise_tv_chambolle(img, weight=0.1, multichannel=True))\n",
    "\n",
    "# Bilateral filter\n",
    "plt.imshow(denoise_bilateral(img, sigma_color=0.05, sigma_spatial=15, multichannel=True))\n",
    "\n",
    "# Wavelet filter\n",
    "plt.imshow(denoise_wavelet(img, multichannel=True))\n",
    "\n",
    "# Flip your images randomly to build a more robust model\n",
    "flipped = np.fliplr(img)  # l = left, r = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing Images in Pytorch\n",
    "import pickle, numpy as np, matplotlib as plt\n",
    "\n",
    "# The dataset consists of 10 000 pictures with the size of (32,32,3)\n",
    "X.shape  #(10000,3072) 3072 = 32*32*3\n",
    "\n",
    "# Reshape\n",
    "X.reshape((-1,3,32,32))  # -1 selects all images(?)\n",
    "\n",
    "# Different ML frameworks expects images to be in different format. Reshape if needed!\n",
    "\n",
    "# Transpose\n",
    "X = X.transpose(0,2,3,1)  # (10000,32,32,3)\n",
    "\n",
    "# Back to original\n",
    "X = X.reshpe(-1,3*32*32)\n",
    "\n",
    "# Center data\n",
    "X = X - X.means(axis=0)  # Subtract the mean value of all pixels ...\n",
    "X = X(np.std(X,axis=0))  # ... and divide by the standard deviation\n",
    "# The resulting images are all centered around 0 and expressed in terms z scores\n",
    "# (standardavvikelser). Helps the performance of the ML models.\n",
    "\n",
    "def show(i):\n",
    "    i = i.reshape((32,32,3))\n",
    "    m, M = i.min(), i.max()  # Min and max value of pixel intensities\n",
    "    plt.imshow((i - m) / (M - m))  # Scales all pixels to be within 0 and 1\n",
    "    plt.show()\n",
    "show(X[6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e614a",
   "metadata": {},
   "source": [
    "#### Whitening\n",
    "Whitening can be used to decorrelate features in our input images. It is the \n",
    "transformation of data in such a way that its covariance matrix is the\n",
    "identity matrix (enhetsmatris).\n",
    "\n",
    "#### Variance vs covariance\n",
    "Wikipedia: Variance refers to the spread of a data set around its mean value,\n",
    "while a covariance refers to the measure of the directional relationship\n",
    "between two random variables.\n",
    "\n",
    "Wikipedia: Om de större värdena hos en variabel i huvudsak korrelerar med de\n",
    "större värden hos den andra variabeln och motsvarande gäller för de mindre\n",
    "värdena (att de uppvisar liknande beteende) är kovariansen positiv.\n",
    "Om de större värdena hos den ena variabeln korrelerar med de mindre hos den\n",
    "andra är kovariansen negativ. Kovariansens tecken visar således tendensen\n",
    "i den linjära relationen mellan variablerna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covarince\n",
    "cov = np.cov(dataset, rowvar=True)\n",
    "\n",
    "# Extract the latent factors in our covariance matrix with Single Value Decomposition\n",
    "U, S, V = np.linalg.svd(cov)\n",
    "\n",
    "# ZCA whitening, the principal components, the sginificant feature\n",
    "epsilon = 1e-5\n",
    "zca_matrix = np.dot(U, np.dot(np.diag(1.0 / np.sqrt(S + epsilon)), U.T))\n",
    "\n",
    "# Multiply the principal components by the original matrix is whitening\n",
    "zca = np.dot(zca_matrix, dataset)\n",
    "\n",
    "# Learn more about why whitening is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch have a bunch of helper classes to transform the images.\n",
    "dir(transforms)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.ToTensor()])\n",
    "\n",
    "# CIFAR10 is a built-in dataset, this code downloads the dataset and transforms the images\n",
    "dataset = torchvision.datasets.CIFAR10(root='./datasets/cifar10/train',\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# Once you hava a dataset in PyTorch, you can load in the data in this dataset\n",
    "# in batches using a DataLoader.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "# Use Python iter function to load in the first batch of images and correspondent labels\n",
    "images_batch, labels_batch = iter(dataloader).next()\n",
    "\n",
    "# Look at the images in the batch\n",
    "img = torchvision.utils.make_grid(images_batch)\n",
    "\n",
    "img.shape\n",
    "# If out is torch.Size([3, something, something]), transpose it so that the channels is last\n",
    "img = np.transpose(img, (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can´t always load all images in a batch at once, but we still have to know the mean and std\n",
    "pop_mean = []\n",
    "pop_std = []\n",
    "\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    numpy_image = data[0].numpy()\n",
    "    \n",
    "    batch_mean = np.mean(numpy_image, axis=(0, 2, 3))  # 0 = batch, 2 = height, 3 = width\n",
    "    batch_std = np.std(numpy_image, axis=(0, 2, 3))\n",
    "    \n",
    "    pop_mean.append(batch_mean)\n",
    "    pop_std.append(batch_std)\n",
    "\n",
    "# Make the lists arrays    \n",
    "pop_mean = np.array(pop_mean)\n",
    "pop_std = np.array(pop_std)\n",
    "\n",
    "# Approximate the mean and standard deviation averages\n",
    "pop_mean = pop_mean.mean(axis=0)\n",
    "pop_std = pop_std.mean(axis=0)\n",
    "\n",
    "# Print the result\n",
    "print(pop_mean)\n",
    "print(pop_std)\n",
    "\n",
    "# Output is\n",
    "# [0.491.., 0.482.., 0.4467..]\n",
    "# [0.238.., 0.234.., 0.2526..]\n",
    "\n",
    "# Examples of somre transforms that we can apply on the input images before we feed them in.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor,\n",
    "    transforms.Normalize(pop_mean, pop_std)  # Normalize is to subtracting the mean and dividing\n",
    "                                             # the images by the standard deviation, often \"standardization\"\n",
    "                                             # outside PyTorch but then it´s slightly different. \n",
    "])\n",
    "\n",
    "# Load the dataset with the specified transforms once again\n",
    "trainset = torchvision.datasets.CIFAR10(root='./datasets/cifar10/train', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# Create a new DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "# Look at one batch\n",
    "images_batch, labels_batch = iter(trainloader).next()\n",
    "images_batch.shape\n",
    "\n",
    "img = torchvision.utils.make_grid(images_batch)\n",
    "img.shape\n",
    "\n",
    "img = np.transpose(img, (1, 2, 0))  # Set the channels in the 3rd dimension if not already there\n",
    "\n",
    "# Scale all pixel value between 0 and 1\n",
    "m, M = img.min(), img.max()  # Check the values\n",
    "img = (1/(abs(m) * M)) * img + 0.5  # The values will be centered around 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da484a",
   "metadata": {},
   "source": [
    "### Introducing Convolutional Neural Networks\n",
    "Every neuron in our eye is stimulated using a local receptive field. CNNs are also designed to have a local receptive field and are used to extract features from input images. Pooling layers in a CNN apply an aggregation function to the input.\n",
    "\n",
    "#### Neural Networks\n",
    "The idea that lower-level neurons pick up granular information which are then put together to understand more complex patterns by higher-level neurons.\n",
    "\n",
    "#### Two Kinds of Layers in CNNs:\n",
    "1. Convolution, local receptive field, reacts only on those pixels that lie within the local receptive field.\n",
    "2. Pooling, responsible for subsampling the input, sampling the input to extract higher level abstractions and make the data more tractable to work with.\n",
    "\n",
    "#### Convolution\n",
    "Allows neurons to focus on a lical receptive field and react to visual stimuli in that field. In this contect, convolutional refers to a mathematical function which basically is a sliding window function that is applied to a matrix. The matrix refers to the input image, a matrix of pixels. The sliding window function is applied to this input image to extract features and is often called a kernel or a filter. It is associated with wieghts and these weight are what help this kernel extract specific kinds of features, detect edges, corners, lines. The function slides over every pixel in the input image. The kernel is applies in an element-wise sliding-window fashion.\n",
    "\n",
    "The matrix (e.g. 6x6) in combination with a kernel (e.g. 3x3) results in a convolution result that is smaller (4x4) than our input image. This is because of the size of the kernel that we´ve chosen. The kernel cannot slide off the edges of the input matrix unless we specify padding.\n",
    "\n",
    "(Is the kernel always built of one 0, one 1, one 0 a.s.o.?)\n",
    "\n",
    "The value of the pixel in the matrix is multiplied by the value of the cell in the kernel that is layered over the matrix, i.e. 1 or 0. The sum of all sums is the value in the first cell of our convolution result table. This convolution kernel is applied to every channel of the input image separately, so if you have a multi-channel image the convolution kernel will slide over the matrix for each channel. \n",
    "\n",
    "#### Convolutional Layers\n",
    "- Convolutional layers zoom in on specific bits of input. \n",
    "- Extract structure and features in the input image (with help of the kernel).\n",
    "- Successive layers aggregate inputs into higher level features. Earlier layers are focusiing on extracting the individual pieces like them in a jigsaw puzzle, later layers put them together. They aggregate and put into higher level features. Pixels >> Lines >> Edges >> Object\n",
    "\n",
    "When you apply a convolution kernel to a pixel matrix, the result of the convolution layer is a feature map/convolution result.\n",
    "\n",
    "#### Kernel Size\n",
    "- Convolutional kernel size usually expressed in terms of width and height of receptive area.\n",
    "- It´s more efficient to use small convolutional kernels.\n",
    "- Stacking two 3x3 kernels is prefereable to one 9x9 kernel.\n",
    "\n",
    "#### Feature Maps\n",
    "A stride is basically the distance between successive receptive fields (the movement of the kernel), a horizontal and a vertical.\n",
    "\n",
    "If we want our feature map to be the same size as our input matrix, we might need 0 padding at the edges. \n",
    "\n",
    "CNNs are sparsely connected neural networks, not densely. Neurons are not connected to all pixels.\n",
    "\n",
    "All neurons in a feature map have the same weight and biases. \n",
    "\n",
    "Two big advantages over DNNs:\n",
    "1. Dramatically fewer parameters to train.\n",
    "2. CNN can recognize feature patterns independent of location. This adds location invariance to our convolutional layer.\n",
    "\n",
    "#### CNNs\n",
    "A convolutional layer can be thought of as a stack of feature maps (same size) which extract different features from the input image which gives it a depth. A CNN is made up of many convolutional layers. Different feature maps have different parameters and extract different features from the input. \n",
    "\n",
    "#### Pooling Layers\n",
    "The input to a pooling layer is essentially a matrix of pixels that is our image. The pooling layer applies a kind of aggregation operation to the input. This aggregation can be the max aggregation (a 2x2 filter (stride = 2 (horizontal and vertical)) for a 4x4 matrix). An alternative is average pooling where pooling layer calculates the average of the input pixels. \n",
    "\n",
    "The pooling filter does not stride pixel by pixel but instead moves one length at some direction. A maximum layer take the max value from the matrix part hovered by the pooling layer and puts it in the pooling result table. \n",
    "\n",
    "Neurons in a pooling layer have no weights or biases.\n",
    "\n",
    "A pooling neuron simply applies some aggregation function to all inputs. Max, sum or average. \n",
    "\n",
    "Why we use them?\n",
    "- Greatly reduce memory usage during training\n",
    "- Mitigate overfitting (via subsampling)\n",
    "- Make NN recognize features independent of location (location invariance)\n",
    "\n",
    "Pooling layers typically act on each channel independently.\n",
    "\n",
    "Usually, output are < input area, but the output depth = input depth.\n",
    "\n",
    "#### CNN Architectures\n",
    "The input will be a batch or list of images which will be passed through convolutional layers, followed by pooling layers followed by other convolutional layers. The order can change between different architectures as well as the number of layers respectively.\n",
    "\n",
    "Each group of convolutional layers is usually followed by a ReLU activation layer. Between the convolutional layers and the ReLU layers you often do a batch nomalization (shifting and scaling the output of your convolutional layer around 0).\n",
    "\n",
    "The output of every layer of the CNN is an image. As the image passes through the layers, the successive outputs are smaller and smaller (thanks to the pooling layers) as well as deeper and deeper (due to feature maps in the convolutional layers). The small but deep output is then fed into a regular, feed-forward neural network. \n",
    "\n",
    "This feed-forward neural network is essentially a fully connected neural network which has a few fully connected layers with the ReLU activation. The feed-forward layer will emit output probabilities for different categories and this is typically the softmax or the log softmax layer. \n",
    "\n",
    "The input to a CNN is an image and the output of a CNN are probabilities. These will be the probabilites corresponding to the different categories in our image classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188c0ee",
   "metadata": {},
   "source": [
    "#### Understand Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad319bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('file/path').convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8eaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c29e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tf.Compose([tf.Resize(400), tf.ToTensor()])  # Resize preserves the original ratio, input in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = transforms(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape\n",
    "# Output is torch.Size([3, 400, 599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 4d matrix (four [])\n",
    "sharpen_kernel = [[\n",
    "    [[0, -1, 0]],\n",
    "    [[-1, 5, -1]],\n",
    "    [[0, -1, 0]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_line_kernel = [[\n",
    "    [[1, 0, -1]],\n",
    "    [[0, 0, 0]],\n",
    "    [[-1, 0, 1]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_line_kernel = [[\n",
    "    [[0, 1, 0]],\n",
    "    [[1, -4, 1]],\n",
    "    [[0, 1, 0]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3050d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_detection_kernel = [[\n",
    "    [[-1, -1, -1]],\n",
    "    [[-1, 8, -1]],\n",
    "    [[-1, -1, -1]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b55366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the convolutional kernel matrix to a torch.Tensor\n",
    "conv_filter = torch.Tensor(sharpen_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddea34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filter.shape\n",
    "# Output is torch.Size([1, 3, 1, 3]) A [] with 1 [[[]]] consisting of\n",
    "# 3 [[]] consisting of 1 [] consisting of 3 values?\n",
    "# 1 = batch, 3 = RGB, 1 = height of the kernel, 3 = width of the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c97547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an additional batch dimension of 1 at axis 0.\n",
    "img_tensor = img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ddca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.shape\n",
    "# Output is torch.Size([1, 3, 400, 599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a83f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sharpen_kernel to our input image\n",
    "conv_tensor = F.conv2d(img_tensor, conv_filter, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228396cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_tensor.shape\n",
    "# Output is torch.Size([1, 1, 400, 597])  # 597: The 3x3 kernel can´t slide over the last to pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the batch layer in the convolution result for an 3d image\n",
    "conv_img = conv_tensor[0, :, :, :]\n",
    "conv_img.shape\n",
    "# Output is torch.Size([1, 400, 597])  # Channel, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8dd776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze out the single dimension at the beginning\n",
    "conv_img = conv_img.numpy().squeeze()\n",
    "conv.img.shape\n",
    "# Output is ([400, 597]) and can be viewed using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24959cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(conv_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4aae0",
   "metadata": {},
   "source": [
    "#### Understand Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f292f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_tensor = pool(conv_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_tensor.shape\n",
    "# Output is torch.Size([1, 1, 200, 298])\n",
    "# The default stride of a pooling layer is typically the size of the pooling kernel.\n",
    "# The output is therefore halfed (compare [400,597] with [200,298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d01956",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_img = pool_tensor[0, :, :, :]\n",
    "pool_img.shape\n",
    "# Output is torch.Size([1, 200, 298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c941084",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_img = pool_img.numpy().squeeze()\n",
    "pool_img.shape\n",
    "# Output is (200, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(pool_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0cea4",
   "metadata": {},
   "source": [
    "### Building Convolutional Neural Networks for Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbfd38",
   "metadata": {},
   "source": [
    "#### Zero-padding, Stride Size \n",
    "##### Narrow vs. Wide Convolution\n",
    "Narrow convolution: Little zero padding; output (feature map/convolution result) narrower than input (image). Default if not specifying zero padding.\n",
    "Wide convolution: Lots of zero padding, output wider than input.\n",
    "\n",
    "With zero padding, every element of the matrix will be passed into filter. You might want to use zero-padding if you have important features located at the edges of your input images. We choose the number of zero columns to pad with. \n",
    "\n",
    "##### Stride Size\n",
    "We can also choose how many columns and rows our kernel should move between each step. A horizontal stride of 1 makes the kernel move one step to the right and a vertical stride of two it moves two steps down. Stride size is an important hyperparameter in CNNs, default value in each direction is 1.\n",
    "\n",
    "#### Batch Normalization\n",
    "When building CNNs you'll often apply batch normalization to the output of your convolutional layers before passing them into the activation function. It mitigates the problem of vanishing and exploding gradients during training.\n",
    "\n",
    "##### Training via Back Propagation\n",
    "Image => Pixels => Edges => Corners => Object Parts => ML-based Classifier\n",
    "\n",
    "The training of a neural network happens via back propagation; the backward pass through our neural network. The forward pass uses the current model's parameters to give you predicted values. You can 1) then compare them with actual values and calculate the error (Classifier). You 2) then use an optimizer which calculates gradients with respect to this error or loss (Object Parts), and 3) then uses these gradients to make backward pass through your neural network (Corners) to 4) update the model parameters (Edges). This is training your model via back propagation.\n",
    "\n",
    "Back propagation fails if:\n",
    "- gradients are vanishing, or\n",
    "- gradients are exploding.\n",
    "\n",
    "##### Vanishing Gradient Problem\n",
    "Gradient descent optimization that we used to train our neural network model involves descending down in this gradient slope to find those model parameters that give us the lowest value of loss. It is possible that while trining your model you'll find that even when you tweak your model parameters, the gradient becomes 0 and stops changing. This don't allow our model to converge and is the vanishing gradient problem.\n",
    "\n",
    "##### Exploding Gradient Problem\n",
    "With this problem the gradient explodes och changes abruptly. This means that instead of descending down the gradient slope to find the smallest value of loss, you're kind of moving up and down with this slope and your model does not converge. \n",
    "\n",
    "##### Coping with Vanishing/Exploding Gradients\n",
    "Neural networks that are very, very deep are especially prone to this problem of vanishing and exploding gradients. \n",
    "It can be mitigated with:\n",
    "- Proper initialization of your model parameters (Xavier and He initialization techniques for example),\n",
    "- Non-saturating activation function,\n",
    "- Batch normalization (reguralization technique that centers your inputs around 0),\n",
    "- Gradient clipping (set a min and max value of the gradient that can exist)\n",
    "\n",
    "##### Batch Normalization\n",
    "Apply to your input just before applying activation function. First normalize inputs (subtracting the mean and dividing by the standard deviation) and then scaling (multiply by constant) and shifting (add constant) the inputs around a specific value. Normalization is a pre-processing technique but is also applied to an intermediate output of our neural network.\n",
    "\n",
    "Batch normlization:\n",
    "- allows much larger learn rate,\n",
    "- reduces overfitting, and\n",
    "- speeds convergence of training.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f2a6d80d",
   "metadata": {},
   "source": [
    "#### Choice of Activation Function\n",
    "Neural networksa re made up of layers of intercinnected neurons. Once you have a fully trained neural network, all the efges which represent interconnetions have weights which help make pedictions. \n",
    "\n",
    "##### Operation of a Single Neuron\n",
    "Neurons are simply mathematical functions. Each neuron applies two simple functions to its inputs:\n",
    "1. Affine Transformation and\n",
    "2. Activation Function.\n",
    "\n",
    "The affine transformation is responsible for learning linear relationships that exist in your data. The affine transformation alone kan ONLY learn LINEAR relationships between the inputs and the output. It is just a weighted sum with a bias added W1x1 + W2x2 + ... + Wnxn + b where W is ??? and x is input. The output is Wx + b.\n",
    "\n",
    "The activation function is responsible for learning non-linear relationships. Wx + b => Activation function => max(Wx + b, 0). The combination of the affine transformation and the activation function can learn any arbitrary (slumpmässig) relationship. \n",
    "\n",
    "##### Activation Function\n",
    "- ReLU \n",
    "- logit\n",
    "- tanh\n",
    "- step\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "All of these function contain an active region with a slope. The choice of activation function is crucial in determining performance. \n",
    "\n",
    "##### Feature Map Size Calculations\n",
    "Formula for dimension calculations: <br>\n",
    "O = (W - K + 2P) / S + 1<br>\n",
    "O = output dimension (height or width)\n",
    "W = height or width of the input dimension (the matrix/image)<br>\n",
    "K = height or width of the kernel size (often squared but not always)  <br>\n",
    "P = padding (if any)<br>\n",
    "S = stride size (horizontal if calculating the width or vertical if calculating the height)\n",
    "\n",
    "#### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec1819a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-88320821d9a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv('datasets(mnist-in-csv/mnist_train.csv)')\n",
    "mnist_test = pd.read_csv('datasets(mnist-in-csv/mnist_test.csv)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data\n",
    "mnist_train = mnist_train.dropna()  # Drops all records which we have missing fields\n",
    "mnist_test = mnist_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "random_sel = mnist_train.sample(8)\n",
    "random_sel.shape\n",
    "# Output is (8, 785), 8 pictures and each record has 785 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd13cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'label' column\n",
    "image_features = random_sel.drop('label', axis=1)\n",
    "\n",
    "# Convert to a 0-1 scale (pixel intensity values = 0 - 255)\n",
    "image_batch = (torch.Tensor(image_features.values / 255.)).reshape((-1, 28, 28))\n",
    "\n",
    "image_batch.shape\n",
    "# Output is torch.Size([8, 28, 28]), 8 images, 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a grid\n",
    "# make_grid converts our images to 3-channel images.\n",
    "grid = torchvision.utils.make_grid(image_batch.unsqueeze(1), nrow=8)\n",
    "grid.shape\n",
    "# Output is torch.Size([3, 32, 242])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the channel dimension the last of the three, \n",
    "# matplotlib expects us to have the channel information in the last dimension\n",
    "plt.figure(figsiza=(12,12))\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff729bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training features (image pixel intensity) ...\n",
    "mnist_train_features = mnist_train.drop('label', axis=1)\n",
    "# ... and the corresponding labels into separate tensors.\n",
    "mnist_train_target = mnist_train['label']\n",
    "\n",
    "mnist_test_features = mnist_test.drop('label', axis=1)\n",
    "mnist_test_target = mnist_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of the image data to torch.Tensors\n",
    "X_train_tensor = torch.tensor(mnist_train_features.values, dtype=torch.float)\n",
    "x_test_tensor = torch.tensor(mnist_test_features.values, dtype=torch.float)\n",
    "\n",
    "Y_train_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(mnist_train_target.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(x_train_tensor.shape)\n",
    "print(Y_test_tensor.shape)\n",
    "print(y_test_tensor.shape)\n",
    "# Output is\n",
    "# torch.Size([60000, 784])\n",
    "# torch.Size([60000])\n",
    "# torch.Size([10000, 784])\n",
    "# torch.Size([10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNs in PyTorch expect images to be fed in (batches, channels, height/width, height/width)\n",
    "# We have to reshape the images.\n",
    "X_train_tensor = X_train_tensor.reshape(-1, 1, 28, 28)\n",
    "\n",
    "x_test_tensor = x_test_tensor.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(x_train_tensor.shape)\n",
    "print(Y_test_tensor.shape)\n",
    "print(y_test_tensor.shape)\n",
    "# Output is\n",
    "# torch.Size([60000, 1, 28, 28])\n",
    "# torch.Size([60000])\n",
    "# torch.Size([10000, 1, 28, 28])\n",
    "# torch.Size([10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573db480",
   "metadata": {},
   "source": [
    "### Building Convolutional Neural Networks for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size is the number of channels in the input image, 1 for grayscales\n",
    "in_size = 1\n",
    "\n",
    "# Two convolutional layers. Refers to the number of feature maps generated by each\n",
    "#  convolutional layer, the depth of the output produced by each layer.\n",
    "hid1_size = 16\n",
    "hid2_size = 32\n",
    "\n",
    "# Output size (the dataset is about digits, and since we have 10 digits it will be equal to 10)\n",
    "out_size = 10\n",
    "\n",
    "# Convolutional kernel size, 5x5 \n",
    "k_conv_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d549db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our custom neural network which inherits from nn.Module base class.\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # The size of this linear layer depends on the size of the outputs of previous layers,\n",
    "        # 512 based on our input image size and the strides and the kernel size. \n",
    "        # We can use the formula discussed earlier (O = (W - K + 2P) / S + 1?) or a more practical approach below.\n",
    "        self.fc = nn.Linear(512, out_size)\n",
    "    \n",
    "    # Takes the input images and applies the convolutional, pooling and linear layers to the input\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # If your're not sure of how many pixels you should have in your linear layer,\n",
    "        # you can simply leave the linear layer out (self.fc = nn.Linear(512, out_size)),\n",
    "        # print out the shapes at every layer of your cnn, and once you know the shape of\n",
    "        # the final output you can then set up your linear layer.\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output of the first layer is passed into the second layer\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # Reshape for the linear layers to a 1d vector \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output will be fed into our fully connected layer which we had set to 512 pixels\n",
    "        out = self.fc(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #return out\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748bee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of this model\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something with using GPU ...\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Output is cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy somehow ...\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889280c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over the tensors for the training and testing images and the corresponding labels\n",
    "# Unsure of whether this comment should be in this or next cell ...\n",
    "ConvNet(\n",
    "    (layer1): Sequential(\n",
    "        (0): Conv2d(1, 16, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )\n",
    "    (layer2): Sequential(\n",
    "        (0): Conv2d(16, 32, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )    \n",
    "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over the tensors for the training and testing images and the corresponding labels\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "x_test_tensor = X_test_tensor.to(device)\n",
    "\n",
    "Y_train_tensor = Y_train_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e58597",
   "metadata": {},
   "source": [
    "#### Training a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss function, can be used when we apply softmax  activationto our linear layer\n",
    "# or work directly with the output of the linear layer.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate an Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1603b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs that the cnn will train\n",
    "num_epochs = 10\n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b497c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to train the CNN\n",
    "for epoch in range(1, num_epochs):\n",
    "    outputs = model(X_train_tensor)\n",
    "    # Calculate the loss from the actual labels vs the predicted labels\n",
    "    loss = criterion(outputs, Y_train_tensor)\n",
    "    \n",
    "    # Zero out the gradients for our model\n",
    "    optimizer.zero_grad()\n",
    "    # Make a backward pass for the neural network\n",
    "    loss.backward()\n",
    "    # Update the model parameters \n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch - %d, loss - #%.5f '%(epoch, loss.item()))\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "# Output is:\n",
    "# Layer 1\n",
    "# torch.Size([60000 (no images), 16 (depth of convolutional layer output), \n",
    "# 12, 12 (images 28x28, kernel =5 => images 24x24, MaxPool size=2 => images 12x12)])\n",
    "\n",
    "# Layer 2\n",
    "# torch.Size([60000, 32, 4, 4]), depth of 32 as of hid2_size\n",
    "\n",
    "# Linear layer (?)\n",
    "# torch.Size([60000, 512 (32*4*4)])\n",
    "\n",
    "# The fully connected linear layer has an output of 10 (n.o. digits)\n",
    "# torch.Size([60000, 10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efabad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the list of the loss values for each epoch\n",
    "x = (range(0, 9))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b66bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to see how this model performs on the test data\n",
    "# eval mode is important when using batch normalization after our convolutional layers.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2938562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNet(\n",
    "    (layer1): Sequential(\n",
    "        (0): Conv2d(1, 16, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )\n",
    "    (layer2): Sequential(\n",
    "        (0): Conv2d(16, 32, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )    \n",
    "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model by calculating the accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7efa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to feed in the test data to our model for evaluation\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y_test = y_test_tensor.cpu().numpy()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(predicted, y_test))\n",
    "    print('Precision: ', precision_score(predicted, y_test, average='weighted'))\n",
    "    print('Recall: ', recall_score(predicted, y_test, average='weighted'))\n",
    "    \n",
    "    # Output is\n",
    "    # Accuracy: 0.7834\n",
    "    # Precision: 0.8176...\n",
    "    # Recall: 0.7834\n",
    "    # With an increased amount of epochs the model will perform better, and we \n",
    "    # can also perform hyperparameter tuning, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed625f",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this layer and hid1_size = 16\n",
    "hid1_size = 8\n",
    "\n",
    "# Continue with ...\n",
    "model = ConvNet()\n",
    "\n",
    "# ... and all of the remaining cells\n",
    "\n",
    "# Output is \n",
    "# Accuracy: 0.7098\n",
    "# Precision: 0.74958...\n",
    "# Recall: 0.7098\n",
    "# Since the result got worse, change back hid1_size = 16\n",
    "\n",
    "# Instead, change the learning_rate from 0.001 to\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Run all cells\n",
    "# Output is\n",
    "# Accuracy: 0.8676\n",
    "# Precision: 0.8784...\n",
    "# Recall: 0.8676\n",
    "\n",
    "# Switch back the learning rate ...\n",
    "learning_rate = 0.001\n",
    "\n",
    "# ... and switch loss method to NLLLoss ...\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# ... and instead of the output linear layer (return out) we apply a log_soxtmax function\n",
    "# to our def forward()\n",
    "# Always use a log_softmax output when using NLLLoss!!!\n",
    "# Run all cells\n",
    "\n",
    "# Output is\n",
    "# Accuracy: 0.761\n",
    "# Precision: 0.80432...\n",
    "# Recall: 0.761\n",
    "\n",
    "# Change back criterion to CrossEntropyLoss() and change the optimizer to Stochastic gradient descent optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# Let def forward() return out again and run all cells\n",
    "\n",
    "# Output is\n",
    "# Accuracy: 0.1261\n",
    "# Precision: 0.30527...\n",
    "# Recall: 0.1261\n",
    "\n",
    "# Change back to the Adam optimizer and let\n",
    "learning_rate = 0.01\n",
    "# Run all cells\n",
    "\n",
    "# Output is\n",
    "# Accuracy: 0.8876\n",
    "# Precision: 0.89328...\n",
    "# Recall: 0.8876"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f6dad",
   "metadata": {},
   "source": [
    "### Optimizing Image Classification with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b7122",
   "metadata": {},
   "source": [
    "#### Perform image classification on the CIFAR-10 dataset using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d647c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the images by normalizing using the mean and standard deviation\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32241274",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P datasets\n",
    "!tar xf datasets/cifar-10-python.tar.gz -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5830a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/cifar-10-batches-py/data_batch_1', 'rb') as input_file:\n",
    "    X = pickle.load(input_file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/cifar-10-batches-py/data_batch_1', 'rb') as input_file:\n",
    "    X = pickle.load(input_file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c845703",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys\n",
    "# Output is dict_keys(['batch_label', 'labels', 'data', 'filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here there was a huge jump in the code to \n",
    "# Print the result\n",
    "print(pop_mean)\n",
    "print(pop_std)\n",
    "\n",
    "# Output is\n",
    "# [0.491.., 0.482.., 0.4467..]\n",
    "# [0.238.., 0.234.., 0.2526..]\n",
    "\n",
    "# Copy the output results and put the values in variables\n",
    "mean = [0.491.., 0.482.., 0.4467..]\n",
    "std = [0.238.., 0.234.., 0.2526..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the transformations that we want to use\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User the same set of transformation to pre-process test data but leave out the flip\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset ias a built-in dataset in PyTorch\n",
    "trainset = torchvision.datasets.CIFAR10(root='datasets/cifar10/train',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And if we need to download the test dataset\n",
    "# CIFAR-10 dataset ias a built-in dataset in PyTorch\n",
    "testset = torchvision.datasets.CIFAR10(root='datasets/cifar10/train',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataLoaders to load this data in batches\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the test DataLoader\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes into which the images can be categorized\n",
    "class_names = trainset.classes\n",
    "\n",
    "print(class_names)\n",
    "# Output is\n",
    "# ['airplane', 'automobile', 'bird', 'cat', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the shape of one batch\n",
    "img, label = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f946d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape\n",
    "# Output is\n",
    "# torch.Size((16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbf6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label\n",
    "# Output is\n",
    "# tensor([3, 4, 4, 7, 3, 8, 0, 3, 5, 8, 2, 6, 2, 5, 7, 0])\n",
    "# The labels are numeric values corresponding to the class names or categories.\n",
    "# Label 0 corresponds to airplane, 3 to cat a.s.o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce8d73",
   "metadata": {},
   "source": [
    "#### Setting up the CNN to perform classification on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 3 # Channels\n",
    "\n",
    "# Convolutional layers\n",
    "hid1_size = 16\n",
    "hid2_size = 32\n",
    "\n",
    "# Linear layers\n",
    "out1_size = 400\n",
    "out2_size = 10  # Corresponding to the number of categories\n",
    "\n",
    "k_conv_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ee4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design our CNN\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.layer3= nn.Sequential(\n",
    "            nn.Linear(hid2_size * k_conv_size * k_conv_size, out1_size),\n",
    "            nn.ReLU(),\n",
    "            # Dropout layers are used in the training mode where we randomly turn off\n",
    "            # certain neurons, forcing others ti learn significant deatures from our input data\n",
    "            # to mitigate the network to overfitting on the training data.\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(out1_size, out2_size)\n",
    "\n",
    "    # Takes the input images and applies the convolutional, pooling and linear layers to the input\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # If your're not sure of how many pixels you should have in your linear layer,\n",
    "        # you can simply leave the linear layer out (self.fc = nn.Linear(512, out_size)),\n",
    "        # print out the shapes at every layer of your cnn, and once you know the shape of\n",
    "        # the final output you can then set up your linear layer.\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output of the first layer is passed into the second layer\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # Reshape for the linear layers to a 1d vector \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output will be fed into our fully connected layer which we had set to 512 pixels\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #return out\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model parameters\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Output is 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the model parameters over to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNet(\n",
    "    (layer1): Sequential(\n",
    "        (0): Conv2d(1, 16, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )\n",
    "    (layer2): Sequential(\n",
    "        (0): Conv2d(16, 32, kernel_size=(5,5), stride=(1,1))\n",
    "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (2): ReLU()\n",
    "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    )    \n",
    "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters to train our model\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(trainloader)\n",
    "num_epochs = 10\n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer for loop is for the number of epochs of training we want to run.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Inner for loop runs over all of the batches in the training data.\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # Calculate the loss from the actual labels vs the predicted labels.\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Zero out the gradients for our model.\n",
    "        optimizer.zero_grad()\n",
    "        # Make a backward pass for the neural network.\n",
    "        loss.backward()\n",
    "        # Update the model parameters .\n",
    "        optimizer.step()\n",
    "        \n",
    "        # For every 200 batches, we'll print out the current epoch, the current step\n",
    "        # and the loss from our model.\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "# Run cell and start training!!! This dataset takes around 10-15 minutes on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss values for every epoch\n",
    "x = (range(1, 11))\n",
    "\n",
    "plt.figure(figsize= (12, 10))\n",
    "\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the fully trained model on the test dataset for prediction.\n",
    "# Make sure that you switch the model over to the eval mode since we have batch\n",
    "# normalization and dropout layers in our model.\n",
    "model.eval()\n",
    "\n",
    "# Turn off gradients during the evaluation process\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the model on the 10000 test images: {}%'\\\n",
    "          .format(100 * correct / total))\n",
    "    \n",
    "# Output is \n",
    "# Accuracy of the model on the 10000 test images: 73.27%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ca043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a prediction using our model on a sample image\n",
    "sample_img, _ = testset[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img.shape\n",
    "# Output is\n",
    "# torch.Size([3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the axes for matplotlib\n",
    "sample_img = np.transpose(sample_img, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the sample image to be between 0 and 1, centered at 0.5\n",
    "m, M = sample_img.min(), sample_img.max()\n",
    "\n",
    "sample_img = (1/(abs(m) * M)) * sample_img + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access test image and corresponding label from the dataset\n",
    "test_img, test_label = testset[23]\n",
    "\n",
    "# Reshape it to a form that our model expet (batch, num_channels, height, width)\n",
    "test_img = test_img.reshape(-1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this image tensor over to a Cuda device before passing it into our model\n",
    "out_predict = model(test_img.to(device))\n",
    "_, predicted = torch.max(out_predict.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5842773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Label: ', test_label)\n",
    "# Output is\n",
    "# Actual Label: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted Label: ', predicted.item())\n",
    "# Output is\n",
    "# Predicted Label: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6fe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class name for {}: {}'.format(predicted.item(), class_names[predicted.item()]))\n",
    "# Output is\n",
    "# Class name for 9: truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608d9d8",
   "metadata": {},
   "source": [
    "##### Changing activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0db634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            # nn.ReLU(),  # Activation layer\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.Tanh(),\n",
    "            nn.ELU();  # Exponential Linear Unit\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.Tanh(),\n",
    "            nn.ELU();\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.layer3= nn.Sequential(\n",
    "            nn.Linear(hid2_size * k_conv_size * k_conv_size, out1_size),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.Tanh(),\n",
    "            nn.ELU();\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(out1_size, out2_size)\n",
    "\n",
    "    # Takes the input images and applies the convolutional, pooling and linear layers to the input\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output of the first layer is passed into the second layer\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # Reshape for the linear layers to a 1d vector \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output will be fed into our fully connected layer which we had set to 512 pixels\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #return out\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82082d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select this cell where you're evaluating your model and select Run All Above Selected Cell.\n",
    "# Wait for the computer to run the cells and then run this cell.\n",
    "model.eval()\n",
    "\n",
    "# Turn off gradients during the evaluation process\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the model on the 10000 test images: {}%'\\\n",
    "          .format(100 * correct / total))\n",
    " \n",
    "\n",
    "# Output with ReLU() is \n",
    "# Accuracy of the model on the 10000 test images: 73.27%\n",
    "\n",
    "# Output with Sigmoid() is \n",
    "# Accuracy of the model on the 10000 test images: 68.02%\n",
    "\n",
    "# Change Sigmoid() to Tanh(), run all cells above this, then this.\n",
    "# Output with Tanh() is \n",
    "# Accuracy of the model on the 10000 test images: 68.81%\n",
    "\n",
    "# Change Tanh() to ELU(), run all cells above this, then this.\n",
    "# Output with ELU() is \n",
    "# Accuracy of the model on the 10000 test images: 71.62%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c37374",
   "metadata": {},
   "source": [
    "##### Apply different pooling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ELU();  # Exponential Linear Unit\n",
    "            # nn.MaxPool2d(kernel_size=2))\n",
    "            # nn.AvgPool2d(kernel_size=2))\n",
    "            nn.LPPool2d(1, kernel_size=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ELU();\n",
    "            # nn.MaxPool2d(kernel_size=2))\n",
    "            # nn.AvgPool2d(kernel_size=2))\n",
    "            nn.LPPool2d(1, kernel_size=2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.layer3= nn.Sequential(\n",
    "            nn.Linear(hid2_size * k_conv_size * k_conv_size, out1_size),\n",
    "            nn.ELU();\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(out1_size, out2_size)\n",
    "\n",
    "    # Takes the input images and applies the convolutional, pooling and linear layers to the input\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output of the first layer is passed into the second layer\n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # Reshape for the linear layers to a 1d vector \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        # The output will be fed into our fully connected layer which we had set to 512 pixels\n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #return out\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all cell until this and then this to train the neural network.\n",
    "\n",
    "# Outer for loop is for the number of epochs of training we want to run.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Inner for loop runs over all of the batches in the training data.\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # Calculate the loss from the actual labels vs the predicted labels.\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Zero out the gradients for our model.\n",
    "        optimizer.zero_grad()\n",
    "        # Make a backward pass for the neural network.\n",
    "        loss.backward()\n",
    "        # Update the model parameters .\n",
    "        optimizer.step()\n",
    "        \n",
    "        # For every 200 batches, we'll print out the current epoch, the current step\n",
    "        # and the loss from our model.\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "# Run cell and start training!!! This dataset takes around 10-15 minutes on the GPU\n",
    "\n",
    "# Then run model.eval() cell to get the accuracy of this new model which is 73.29%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8bb80",
   "metadata": {},
   "source": [
    "Before we change pooling layer function we choose Edit => Clear All Outputs and\n",
    "scroll back to class ConvNet(nn.Module):\n",
    "\n",
    "PyTorch allows us to tweak our layer to be somewhere between MaxPool and AvgPool.\n",
    "This is called the LPPool. 2D power average pooling: p = infinity gives Max pooling\n",
    "and p = 1 gives Average Pooling. Other values between 1 and infinity allows us to tweak\n",
    "somewhere between MaxPool and AvgPool.\n",
    "\n",
    "Scroll down and select the cell above model.eval() (x = (range(1, 11)) and choose Run All Above Selected Cell. When the model has trained, run model.eval() to get the accuracy. Now choose different values for LPPool and see how the model changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8acbb4",
   "metadata": {},
   "source": [
    "##### Apply different convolution kernel sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82264357",
   "metadata": {},
   "source": [
    "Just for showing the problem ...\n",
    "\n",
    "Start with Edit => Clear All Outputs, refresh the page and restart the kernel.\n",
    "\n",
    "Run all cells until you come to the cell where your're defining the variables for your CNN (in_size = 3 ...). Let set the kernel size to 6.\n",
    "\n",
    "The kernel size has to be such that your output feature map as integer values for dimensions, height. They have to fit in with the size of the input image that you're working on, and the size of the kernel of the pooling layer that you're using. \n",
    "\n",
    "Run all cells until you come to for epoch in rang(num_epochs): Here you will get an error when running the cell due to a mismatch in kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7174ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e15018",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.491.., 0.482.., 0.4467..]\n",
    "std = [0.238.., 0.234.., 0.2526..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90720a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the same dataset but we'll resixe the input images so that\n",
    "# they are 128x128 images, larger images.\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalization(mean, std)  # Should it be Normalize???\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36743824",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalization(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb979009",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.dataset.CIFAR10(root=....)\n",
    "\n",
    "testset = torchvision...\n",
    "\n",
    "trainloader = torch.utils...\n",
    "\n",
    "tesloader = torch.utils..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables that hold information about the CNN that we are about to set up.\n",
    "in_size = 3  # RGB = 3 channels\n",
    "\n",
    "# Convolution and pooling layers\n",
    "hid1_size = 16\n",
    "hid2_size = 32\n",
    "hid3_size = 64\n",
    "\n",
    "# Fully connected layers (linear?)\n",
    "out1_size = 2000\n",
    "out2_size = 10\n",
    "\n",
    "k_conv_soze = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Conv2D output will be 120x120, and the MaxPool2d output will be 60x60\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.ReLU(); \n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Conv2D output will be 52x52 and MaxPool2D will be 26x26\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(); \n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Conv2D output will be 18x18 and MaxPool2D will be 9x9, equal to the size \n",
    "        # of our convolution kernel\n",
    "        self.layer3= nn.Sequential(\n",
    "            nn.Conv2d(hid2_size, hid3_size, k_conv_size), \n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(); \n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # The number of neurons in the 1st fully connected layer is the number of\n",
    "        # the feature maps produced by the last convolutional layer (hid3_size*the size\n",
    "        # of every feature map, that is 9x9)\n",
    "        self.layer4= nn.Sequential(\n",
    "            nn.Linear(hid3_size * k_conv_size * k_conv_size, out1_size),\n",
    "            nn.ReLU();\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(out1_size, out2_size)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        print(out.shape)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #return out\n",
    "        return F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107af847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "device = torch.device...\n",
    "model.to(device)\n",
    "learning_rate =\n",
    "criterion =\n",
    "optimizer ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea95cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the cell where we train our CNN\n",
    "total_step =...\n",
    "\n",
    "for epoch in range...\n",
    "\n",
    "# Run these cells and observe the output sizes after each layer. Check that the\n",
    "# output matches what we are expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fda8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (range...)\n",
    "\n",
    "plt.figure... ('Step' and 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4504b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()...\n",
    "\n",
    "# Accuracy of this models is 75.76%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a526f",
   "metadata": {},
   "source": [
    "### Performing Image Classification with Pre-trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f59dc",
   "metadata": {},
   "source": [
    "##### Transfer Learning\n",
    "The practice of re-using a trained neural network that solves a problem similar to yours, freezing the lower layers and only re-training the higher layers.\n",
    "\n",
    "Reusing a neural network can involve reusing the architecture of the model as well as the pre-trained weights. Transfer learning works only for specific use cases that are common, such as image recognition and classification or language translation. \n",
    "\n",
    "The initial layers of any neural network focus on granilar details of the input. Lower layers mostly perform feature extraction. With transfer learning we can freeze the lower layers so that they continue to perform feature extraction. The higher/later layers are more specific to your problem. Numbers of outputs likely changes too.\n",
    "\n",
    "Benefits of Transer Learning:\n",
    "- Ride on the shoulders of giants\n",
    "- NN architecture\n",
    "- Choice of initialization\n",
    "- Activation functions\n",
    "- Number and density of layers\n",
    "\n",
    "Transfer learning works very well if you have a small dataset on which you want to train your model. You can do more with less. If we have little training data we might overfit on the data. It is faster and cheaper: We have smaller training data the we need to feed into our model and we only need to train a few layer.\n",
    "\n",
    "In PyTorch, there are supprort for several famous NN architectures, specially for image classification (torchvision.models is Alexnet, VGG, ResNet, Inception ...). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3720e",
   "metadata": {},
   "source": [
    "##### Using the ResNet-18 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8521f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d2b93",
   "metadata": {},
   "source": [
    "Pre-trained models in PyTorch expect images of at least 224x224, pixel values in the range (0,1) and normalized using these values: https://pytorch.org/docs/stable/torchvision/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizeCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa98c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6def397",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/cifar10/train'\n",
    "batch_size = 8\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root=data_dir,\n",
    "                           train=True,\n",
    "                           download=True,\n",
    "                           transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets.CIFAR10(root=data_dir,\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': trainloader,\n",
    "    'test': testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This holds the number of batches that exist in training and test data.\n",
    "dataset_sizes = {'train': len(trainloader), 'test': len(testloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceefed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = trainset.classes\n",
    "\n",
    "print(class_names)\n",
    "# Output is the image categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to use the pre-trained weight of the ResNet model, which we do,\n",
    "# we set pretrained equal to True. If we only want to use the architecture of\n",
    "# the model but not the pretrained weights, we set pretrained equal False.\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# In order to use this model with our dataset, we'll need to change the last\n",
    "# layer of this model. All pretrained models in PyTorch have been trained on\n",
    "# the ImageNet dataset which has images classified into 1000 categories. The \n",
    "# out_feature of the last linear layer is therefore equal to 1000. We need to \n",
    "# change this number to the number of categories in our dataset.\n",
    "\n",
    "# There is a key associated with each layer in our model, this layer can be\n",
    "# accessed using fc (fully connected).\n",
    "\n",
    "# The first thing to do is to freeze the model parameters of all the layers in \n",
    "# the pretrained model.\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "    \n",
    "# Then replace the last linear layer. The in_features is the number of features\n",
    "# thar we need to pass into the input of this layer.\n",
    "num_ftrs = model.fc.in_features\n",
    "num_ftrs\n",
    "# Outpus is 512\n",
    "# Instantiate/replace a new linear layer as the last fully connected layer of the \n",
    "# ResNet model with the output size = number of output labels in our dataset.\n",
    "model.fc = nn.Linear(num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the CPU if only the CPU is available on your machine, otherwise the GPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over our model parameters to the device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeba4c",
   "metadata": {},
   "source": [
    "#### The Train Function to Find the Best Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is set ut, let's instantiate and tran the model parameters\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c752928",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters to train the model\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a37f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(trainloader)\n",
    "num_epochs = 10\n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11226094",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        # For every batch we'll need to move the image tensors and corresponding ylabels\n",
    "        # over to the device before we can feed them into our ML model and calculate the loss.\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # The backward pass that calculates the gradients for the model weights\n",
    "        # and then updates the gradients using the optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 2000 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                 .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9af8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (range(1, 11))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Step')\n",
    "plt.ylbel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c875e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the model on the 10000 test images: {}%'\\\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9545e6",
   "metadata": {},
   "source": [
    "#### Predictions Using Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer_ft,\n",
    "                   exp_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help function to function below\n",
    "def imshow(inp, title):\n",
    "    \n",
    "    inp = inp.cpu().numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.pause(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up torch.no_grad so that we don't calculate gradients in the prediction pass\n",
    "with torch.no_grad():\n",
    "    \n",
    "    inputs, labels = iter(dataloaders['test']).next()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inp = torchvision.utils.make_grid(inputs)\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    _, preds = torh.max(outputs, 1)\n",
    "    \n",
    "    for j in range(len(inputs)):\n",
    "        inp = inputs.data[j]\n",
    "        imshow(inp, 'predicted: ' + class_names[preds[j]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
